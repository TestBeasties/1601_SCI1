# Results {#results -}

This document itself is the primary result of the project. It will be shared with members of the General Education Committee, Academic Senate, and the Department of Biological Sciences at Ferris State University. Their comments and suggestions will be included in the Discussion.

```{r ttest, echo=FALSE, message=FALSE, comment=NA, results='asis'}
scoreResults <- t.test(myData$SCI1, mu=2.6)
```

## Summary statistics {-}
A total of `r length(myData$SCI1)` student performances on exam 1 were collected over `r max(myData$Order)` semesters of instruction. Student scores were converted to rubric scores as described above. The overall average rubric score for all students and semesters was `r round(mean(myData$SCI1),2)`. The mode and median scores were `r round(mfv(myData$SCI1)[1],2)` and `r round(median(myData$SCI1),2)`, respectively. The average was not statistically different from the threshold score for competence (2.6) as evaluated with a one-value, two-tailed t-test (t=`r round(scoreResults$statistic,2)`, df=`r round(scoreResults$parameter,2)`, p=`r format(scoreResults$p.value, digits=2)`). The effect size for the difference between the average and the threshold was tiny (d=`r round(scoreResults$statistic/sqrt(scoreResults$parameter),2)`). We can infer from this that the overall average rubric score is not practically different than the threshold score.

```{r histogram, echo=FALSE, message=FALSE, results='hide', fig.width=6, fig.align='center', fig.cap="A histogram of the distribution of individual rubric score frequencies over all thirteen semesters."}
distribution <- table(myData$SCI1)
barplot(distribution, ylim=c(0,500), las=1, xlab="", ylab="Overall Frequency", axis.lty = 1, col="firebrick", cex.axis = 0.85, cex.lab = 0.85, cex.names=0.85)
mtext(side = 1, text = "Rubric Score on SCI1", line = 1.8, cex=0.85)
```

The distribution of all rubric scores is shown in Figure \@ref(fig:histogram). This distribution exhibited a moderate negative skew (skew = `r round(skewness(myData$SCI1, na.rm = TRUE),2)`). This result may simply indicate that the teaching, materials, and student learning are all functioning well when the scores are viewed in aggregate. A total of `r sum(myData$SCI1 >= 3)` students (`r round(sum(myData$SCI1 >= 3)/length(myData$SCI1)*100,1)`%) met or exceeded the competence threshold over the semesters investigated.

The distribution of rubric scores by semester is shown in Figure \@ref(fig:barplot). There are rather obvious differences in both the distribution of rubric scores and class sizes between semesters. A one-way ANOVA was used to compare the rubric scores by semester (Table \@ref(tab:anova)). Unsurprisingly, there were statistically significant differences between semester scores. Semester of instruction, however, explained a relatively small amount of the overall variance ($\eta^2$ = `r round(anova(lm(SCI1 ~ Semester, data = myData))[1,2]/sum(anova(lm(SCI1 ~ Semester, data = myData))[,2]),2)`).

```{r barplot, echo=FALSE, message=FALSE, fig.cap="A barplot showing the distribution of rubric scores broken down by semester.", fig.width=6, fig.align='center'}
bySemester <- as.matrix(table(myData$SCI1,myData$Order))
bySemester <- bySemester[,ncol(bySemester):1]
semesterTotals <- apply(bySemester, 2, sum)
for(dummy in 1:ncol(bySemester)){
	bySemester[,dummy] <- bySemester[,dummy]/semesterTotals[dummy]*100
}
myLabels <- c("Fall 2015","Spring 2015","Fall 2014","Spring 2014","Fall 2013","Spring 2013","Fall 2012","Spring 2012","Fall 2011","Spring 2011","Fall 2010","Spring 2010","Fall 2009")
#col <- c("firebrick","red","yellow","aquamarine","darkgreen")
col <- c("#a50f15","#de2d26","#fb6a4a","#fcae91","#fee5d9")

par(mar=c(4,8,3,2)+0.1)
barplot(as.matrix(bySemester),
	col=col,
	horiz = TRUE,
	xlab="",
	ylab="",
	names.arg=myLabels,
	xlim=c(0,100),
	las=1,
	cex.axis = 0.75,
	cex.lab=0.75,
	cex.names = 0.75)
mtext("Relative Frequency of Rubric Scores",side=1,line=2, cex = 0.75)
mtext("Semester",side=2,line=5, cex = 0.75)
add_legend("top",                             # Add a legend to the plot
       legend=c("0","1","2","3","4"),          # Text for the legend
       fill=col,                               # Fill for boxes of the legend
       title="Rubric Score",
       bty="n",
       cex = 0.75,
       horiz = TRUE)                           # Fill for boxes of the legend
```


```{r anova, echo=FALSE, results='asis'}
myModel <- lm(SCI1 ~ Semester, data = myData)

knitr::kable(anova(myModel),caption="One-way ANOVA analysis of scores by semester")
```

## Meta-analysis {-}
Meta-analysis of the student performance was performed using R [@TQMP11-1-37]. This analysis resulted in a weighted average of rubric scores. This value was calculated using formula \@ref(eq:weightX). The value $X_{i}$ average rubric scores for the semesters, while $P_{i}$ is the weighting factor (student enrollment).

\begin{equation}
\bar{X}_w = \frac{\sum X_i P_i}{\sum P_i}
(\#eq:weightX)
\end{equation}

The confidence interval for the weighted mean was calculated using the weighted variance. However, the weighted variance is actually not simple to calculate. Several different methods have been compared to bootstrapping [@Gatz1995a]. The most accurate method was initially described by Cochran [@Cochran1977] and that one was used in this study. The calculation to obtain the weighted variance is shown in formula \@ref(eq:weightV).

\begin{equation}
\begin{split}
(SEM_w)^2 = \frac{n}{(n-1)(\sum P_i)^2}\big[ \sum(P_i X_i - \bar{P}\bar{X}_w)^2 \\
- 2\bar{X}_w \sum(P_i - \bar{P})(P_i X_ i - \bar{P} \bar{X}_w) + \bar{X}_w^2 \sum(P_i - \bar{P})^2 \big]
\end{split}
(\#eq:weightV)
\end{equation}


```{r forest, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.cap="A forest plot of the average scores for each semester with a weighted mean estimate for the entire period investigated. Error bars indicate the 95% confidence intervals."}
myData$SCI1 <- as.numeric(myData$SCI1)

dataTable <- myData %>%
	group_by(Order) %>%
	summarise(n = length(SCI1), mean = mean(SCI1), lower = mean(SCI1)-(1.96*sd(SCI1)/sqrt(length(SCI1))), upper = mean(SCI1)+(1.96*sd(SCI1)/sqrt(length(SCI1))))
dataTable$Order <- NULL
myWeighted <- weighted.var.se(dataTable$mean,dataTable$n)
dataTable$n <- NULL
dataTable <- rbind(dataTable,myWeighted)
nullHeadings <- c(NA,NA,NA)
dataTable <- rbind(nullHeadings,dataTable)

textTable <- myData %>%
	group_by(Order) %>%
	summarize(Semester = Semester[1], Prefix = Prefix[1], Level = Level[1], Outcome = "SCI1", N = length(SCI1), Mean = format(round(mean(SCI1),digits=2),nsmall=2))
textTable$Order <- NULL
textTable$Semester <- as.character(textTable$Semester)
textTable$Prefix <- as.character(textTable$Prefix)
textTable$Level <- as.character(textTable$Level)
headings <- c("Semester","Prefix","Level","Outcome","N","Mean")
textTable <- rbind(headings,textTable)
theSummary <- c("Weighted average",NA,NA,NA,NA,myWeighted)
textTable <- rbind(textTable,theSummary)

align <- c("c","c","c","c","c","c")

forestplot(textTable, dataTable,
	   new_page = FALSE,                             # Image on one page
	   is.summary=c(TRUE,rep(FALSE,13),TRUE),        # Bold for heading and summary lines
	   boxsize = .3,                                 # Set symbol size
	   xlog=FALSE,                                   # Linear scale
	   xticks = c(0,1,2,3,4),                        # Ticks at the rubric values
	   zero = 2.6,                                   # Set threshold value
	   grid = gpar(lty=3, col="#333333", lwd=1.25),  # Make vertical lines gray dots
	   xlab = "\nMean rubric score Â± 95% CI",        # Label x-axis
	   #title = "Performance on Scientific Understanding Outcome #1 Based Upon Lecture Exam 1",
	   align = align,                                # Center all text columns in table
	   colgap = unit(1, 'mm'),                       # Tighten up the columns
	   graphwidth = unit(70, 'mm'),                  # Make the plot 80mm wide
	   graph.pos=ncol(textTable),                    # Move average values after the plot
	   hrzl_lines = TRUE,                            # Add horizontal lines
	   txt_gp = fpTxtGp(label=gpar(cex=.75), xlab = gpar(cex=0.75), ticks = gpar(cex=0.75)),
	   col=fpColors(box="firebrick",line="black", summary="firebrick", zero="gray50"))
```

```{r weightedT, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
averages <- myData %>%
	group_by(Order) %>%
	summarise(n = length(SCI1), mean = mean(SCI1))

weightedT <- wtd.t.test(averages$mean, 2.6, averages$n)
```

A forest plot of the meta-analysis is shown in Figure \@ref(fig:forest). In this representation, each semester is illustrated as a separate line. The mean and 95% confidence intervals for each semester are plotted in the right panel and their associated meta-data are given in the table to the left. The weighted average of all the data is plotted at the bottom of the figure. The width of the diamond indicates the 95% confidence interval.

The rubric scale can be conceptually divided into five areas as shown in Table \@ref(tab:regions). Of the `r max(myData$Order)` semesters, `r sum(averages$mean >= 2.6)` fell in the proficient range, `r sum(averages$mean >= 1.8 & averages$mean < 2.6)` fell in the developing range, and `r sum(averages$mean < 1.8)` fell in the beginning range. The weighted mean score, `r myWeighted[1]`, was not significantly different from the threshold of competence as judged by a weighted, one-factor, two-tailed t-test (t=`r round(weightedT$coefficients[1],2)`, df=`r weightedT$coefficients[2]`, p=`r round(weightedT$coefficients[3],2)`). We can conclude that the weighted average score is practically equivalent to the competency threshold score.

```{r regions, echo=FALSE, out.width=4}
Average <- c("0.00 to 0.99","1.00 to 1.79","1.80 to 2.59","2.60 to 3.39","3.40 to 4.00")
Interpretation <- c("Unsatisfactory","Beginning","Developing","Proficient","Advanced")
newTable <- as.data.frame(cbind(Average,Interpretation))
names(newTable) <- c("Average Score","Interpretation")
knitr::kable(newTable, caption="Interpretation of average rubric scores", align=c("l","r"), booktabs=TRUE)
```
